<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>Lei Li - HKU-NLP</title>
  <meta name="description" content="Lei Li is a PhD student at HKU-NLP working on multimodal large language models and in-context learning. Best Paper Award at EMNLP 2023.">
  <meta name="author" content="Lei Li">
  <meta name="keywords" content="Lei Li, HKU, NLP, multimodal, large language models, in-context learning">
  <!-- Open Graph -->
  <meta property="og:title" content="Lei Li - HKU-NLP">
  <meta property="og:description" content="PhD student at HKU-NLP working on multimodal large language models and in-context learning.">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://lilei-nlp.github.io/">
  <meta property="og:image" content="https://lilei-nlp.github.io/assets/images/deecamp.jpeg">
  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="@_TobiasLee">
  <meta name="twitter:title" content="Lei Li - HKU-NLP">
  <meta name="twitter:description" content="PhD student at HKU-NLP working on multimodal LLMs and in-context learning.">
  <meta name="twitter:image" content="https://lilei-nlp.github.io/assets/images/deecamp.jpeg">
  <!-- Assets -->
  <link rel="icon" href="assets/images/deecamp.jpeg" type="image/jpeg">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="docs/style.css">
</head>
<body>
  <div class="wrapper">

    <!-- ==================== SIDEBAR ==================== -->
    <header>
      <h1 id="top">Lei Li</h1>
      <p class="subtitle">PhD Student<br>The University of Hong Kong</p>
      <img src="assets/images/deecamp.jpeg" alt="Lei Li">
      <ul class="contact">
        <li><i class="fa-solid fa-envelope"></i> nlp [dot] lilei [at] gmail [dot] com</li>
        <li><i class="ai ai-google-scholar"></i> <a href="https://scholar.google.com/citations?user=MeV4GGsAAAAJ">Google Scholar</a></li>
        <li><i class="fa-brands fa-github"></i> <a href="https://github.com/TobiasLee">GitHub</a></li>
        <li><i class="fa-brands fa-x-twitter"></i> <a href="https://x.com/_TobiasLee">@_TobiasLee</a></li>
        <li><i class="fa-solid fa-file-pdf"></i> <a href="assets/LILEI_CV_2602.pdf">Curriculum Vitae</a></li>
      </ul>
    </header>

    <!-- ==================== MAIN CONTENT ==================== -->
    <section>

      <!-- Bio -->
      <p class="bio">
        I am a PhD student in the <a href="https://hkunlp.github.io/">HKU-NLP</a> group, co-supervised by
        Prof. <a class="adv" href="https://ikekonglp.github.io/">Lingpeng Kong</a> and
        Prof. <a class="adv" href="https://leuchine.github.io/">Qi Liu</a>.
        I completed my master's degree at Peking University advised by
        Prof. <a class="adv" href="https://xusun26.github.io/">Xu Sun</a>
        and my bachelor's degree at Xidian University.
      </p>
      <p class="bio">
        My research focuses on:
        (i) developing frontier multimodal large language models
        (<a href="https://arxiv.org/abs/2506.03569">MiMo-VL</a>,
         <a href="https://arxiv.org/abs/2404.12387">Reka Flash</a>);
        (ii) understanding the fundamental mechanisms of LLMs and MLLMs
        (<a href="https://arxiv.org/abs/2305.14160">In-context Learning</a>,
         <a href="https://arxiv.org/abs/2305.17926">LLMs-as-a-Judge</a>).
        I'm always happy to discuss potential collaborations&mdash;feel free to reach out!
      </p>

      <!-- News -->
      <h2>News</h2>
      <ul class="news">
        <li><span class="date">[2026/01]</span> Honored to receive the inaugural <strong>Tencent Qingyun Scholarship</strong> (15 awardees per year nationwide)!</li>
        <li><span class="date">[2025/11]</span> One paper accepted at AAAI 2026. Received <span class="highlight-gold">Outstanding Area Chair Award</span> at EMNLP 2025!</li>
        <li><span class="date">[2025/08]</span> Two papers accepted by EMNLP 2025 and one by ACM MM 2025!</li>
        <li><span class="date">[2025/02]</span> VL-RewardBench and Video-MME accepted by CVPR 2025, both Highlight (Top 3%)!</li>
        <li><span class="date">[2025/01]</span> Four papers accepted by ICLR 2025, see you in Singapore!</li>
        <li><span class="date">[2024/10]</span> <a href="https://aclanthology.org/2024.acl-long.511/">FairEval</a> selected as <a href="https://www.paperdigest.org/2024/09/most-influential-acl-papers-2024-09/">Most Influential Paper of ACL 2024</a>!</li>
        <li><span class="date">[2023/12]</span> <a href="https://aclanthology.org/2023.emnlp-main.609/">Label Words are Anchors</a> won <span class="highlight-red">Best Long Paper Award</span> at EMNLP 2023!</li>
      </ul>

      <!-- Education -->
      <h2>Education</h2>
      <ul class="exp">
        <li>
          <span class="role">PhD Student</span>, The University of Hong Kong
          <span class="period">&middot; Sept. 2023 &ndash; Now</span>
        </li>
        <li>
          <span class="role">MSc</span> in Computer Science, Peking University
          <span class="period">&middot; Sept. 2020 &ndash; July 2023</span>
        </li>
        <li>
          <span class="role">BE</span> in Software Engineering, Xidian University
          <span class="period">&middot; Sept. 2016 &ndash; Jul. 2020</span>
        </li>
      </ul>

      <!-- Experience -->
      <h2>Experience</h2>
      <ul class="exp">
        <li>
          <span class="org"><a href="https://huggingface.co/XiaomiMiMo">Xiaomi LLM-Core Team</a></span>, Core Member
          <span class="period">&middot; Jan. 2025 &ndash; Now</span>
        </li>
        <li>
          <span class="org"><a href="https://reka.ai/">Reka AI</a></span>, Multi-modal LLM R&amp;D Intern
          <span class="period">&middot; Jul. 2023 &ndash; Dec. 2024</span>
        </li>
        <li>
          <span class="org">Shanghai AI Lab</span>, Research Intern
          <span class="period">&middot; Jul. 2022 &ndash; Jun. 2023</span>
          <span class="mentor">Mentor: Dr. <a class="ppl" href="https://jingjingxu.com/">Jingjing Xu</a></span>
        </li>
        <li>
          <span class="org">WeChat AI</span>, Research Intern
          <span class="period">&middot; Apr. 2020 &ndash; Nov. 2021</span>
          <span class="mentor">Mentor: Dr. <a class="ppl" href="https://linyankai.github.io/">Yankai Lin</a> and Dr. <a class="ppl" href="https://www.lpeng.net/">Peng Li</a></span>
        </li>
      </ul>

      <!-- Selected Publications -->
      <h2>Selected Publications</h2>
      <p class="note">(#: Equal Contribution)</p>

      <h3>Multimodal LLMs</h3>
      <ul class="pub-list">
        <li>
          <a class="title-link" href="https://arxiv.org/abs/2411.17451">VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models</a>
          <span class="authors"><strong>Lei Li #</strong>, Yuancheng Wei #, Zhihui Xie #, Xuqing Yang #, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, Qi Liu</span>
          <span class="venue">CVPR 2025</span> <span class="highlight">(Highlight, Top 3%)</span>
          <span class="links">
            <a class="code" href="https://arxiv.org/abs/2411.17451">[Paper]</a>
            <a class="code" href="https://vl-rewardbench.github.io/">[Project]</a>
          </span>
        </li>
        <li>
          <a class="title-link" href="https://arxiv.org/abs/2405.21075">Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</a>
          <span class="authors">Chaoyou Fu, Yuhan Dai, Yongdong Luo, <strong>Lei Li</strong>, Shuhuai Ren, Renrui Zhang et al.</span>
          <span class="venue">CVPR 2025</span> <span class="highlight">(Highlight, Top 3%, <a class="award" href="https://resources.paperdigest.org/2025/09/most-influential-cvpr-papers-2025-09-version/">Most Influential Paper</a>)</span>
          <span class="links">
            <a class="code" href="https://arxiv.org/abs/2405.21075">[Paper]</a>
            <a class="code" href="https://video-mme.github.io/home_page.html">[Project]</a>
          </span>
        </li>
        <li>
          <a class="title-link" href="https://arxiv.org/abs/2410.06166">Temporal Reasoning Transfer from Text to Video</a>
          <span class="authors"><strong>Lei Li #</strong>, Yuanxin Liu #, Linli Yao, Peiyuan Zhang, Chenxin An, Lean Wang, Xu Sun, Lingpeng Kong, Qi Liu</span>
          <span class="venue">ICLR 2025</span>
          <span class="links">
            <a class="code" href="https://arxiv.org/abs/2410.06166">[Paper]</a>
            <a class="code" href="https://video-t3.github.io/">[Project]</a>
          </span>
        </li>
        <li>
          <a class="title-link" href="https://vlf-silkie.github.io/">VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment</a>
          <span class="authors"><strong>Lei Li #</strong>, Zhihui Xie #, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, Qi Liu</span>
          <span class="venue">EMNLP 2024</span>
          <span class="links">
            <a class="code" href="https://vlf-silkie.github.io/">[Project]</a>
          </span>
        </li>
        <li>
          <a class="title-link" href="https://arxiv.org/abs/2311.17404">VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models</a>
          <span class="authors">Shicheng Li, <strong>Lei Li</strong>, Shuhuai Ren, Yuanxin Liu, Yi Liu, Rundong Gao, Xu Sun, Lu Hou</span>
          <span class="venue">ECCV 2024</span>
          <span class="links">
            <a class="code" href="https://arxiv.org/abs/2311.17404">[Paper]</a>
            <a class="code" href="https://huggingface.co/datasets/lscpku/VITATECS">[Dataset]</a>
          </span>
        </li>
        <li>
          <a class="title-link" href="https://mm-arxiv.github.io/">Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models</a>
          <span class="authors"><strong>Lei Li #</strong>, Yuqi Wang #, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, Qi Liu</span>
          <span class="venue">ACL 2024</span>
          <span class="links">
            <a class="code" href="https://mm-arxiv.github.io/">[Project]</a>
          </span>
        </li>
        <li>
          <a class="title-link" href="https://arxiv.org/pdf/2305.14057.pdf">Can Language Models Understand Physical Concepts?</a>
          <span class="authors"><strong>Lei Li</strong>, Jingjing Xu, Qingxiu Dong, Ce Zheng, Qi Liu, Lingpeng Kong, Xu Sun</span>
          <span class="venue">EMNLP 2023</span>
          <span class="links">
            <a class="code" href="https://arxiv.org/pdf/2305.14057.pdf">[Paper]</a>
            <a class="code" href="https://github.com/TobiasLee/VEC">[Dataset]</a>
          </span>
        </li>
      </ul>

      <h3>Emerging Capabilities of LLMs</h3>
      <ul class="pub-list">
        <li>
          <a class="title-link" href="https://arxiv.org/pdf/2301.00234.pdf">A Survey for In-context Learning</a>
          <span class="authors">Qingxiu Dong, <strong>Lei Li</strong>, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, Xu Sun, Lei Li, Zhifang Sui</span>
          <span class="venue">EMNLP 2024</span> <span class="highlight">(<a class="award" href="https://resources.paperdigest.org/2025/03/most-influential-emnlp-papers-2025-03-version/">Most Influential Paper</a>)</span>
          <span class="links">
            <a class="code" href="https://arxiv.org/pdf/2301.00234.pdf">[Paper]</a>
            <a class="code" href="https://github.com/dqxiu/ICL_PaperList">[Paper List]</a>
          </span>
        </li>
        <li>
          <a class="title-link" href="https://arxiv.org/abs/2305.17926">Large Language Models are not Fair Evaluators</a>
          <span class="authors">Peiyi Wang, <strong>Lei Li</strong>, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, Zhifang Sui</span>
          <span class="venue">ACL 2024</span> <span class="highlight">(<a class="award" href="https://resources.paperdigest.org/2025/03/most-influential-acl-papers-2025-03-version/">Most Influential Paper</a>)</span>
          <span class="links">
            <a class="code" href="https://arxiv.org/abs/2305.17926">[Paper]</a>
            <a class="code" href="https://github.com/i-Eval/FairEval">[Code]</a>
          </span>
        </li>
        <li>
          <a class="title-link" href="https://arxiv.org/pdf/2305.14160.pdf">Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning</a>
          <span class="authors">Lean Wang, <strong>Lei Li</strong>, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun</span>
          <span class="venue">EMNLP 2023</span> <span class="highlight">(Best Long Paper Award)</span>
          <span class="links">
            <a class="code" href="https://arxiv.org/pdf/2305.14160.pdf">[Paper]</a>
            <a class="code" href="https://github.com/lancopku/label-words-are-anchors">[Code]</a>
          </span>
        </li>
      </ul>

      <button class="toggle-btn" id="older-pub-btn" type="button">&#9654; Older Publications</button>
      <div id="older-pub" style="display:none;">

        <h3>Efficient Pre-trained Language Models</h3>
        <ul class="pub-list">
          <li>
            <a class="title-link" href="https://arxiv.org/pdf/2208.07232.pdf">Distributional Correlation-Aware Knowledge Distillation for Stock Trading Volume Prediction</a>
            <span class="authors"><strong>Lei Li</strong>, Zhiyuan Zhang, Ruihan Bao, Keiko Harimoto, Xu Sun</span>
            <span class="venue">ECML-PKDD 2022 (Oral)</span>
            <span class="links">
              <a class="code" href="https://arxiv.org/pdf/2208.07232.pdf">[Paper]</a>
              <a class="code" href="https://github.com/lancopku/DCKD">[Code]</a>
            </span>
          </li>
          <li>
            <a class="title-link" href="https://aclanthology.org/2022.findings-emnlp.477">From Mimicking to Integrating: Knowledge Integration for Pre-Trained Language Models</a>
            <span class="authors"><strong>Lei Li</strong>, Yankai Lin, Xuancheng Ren, Guangxiang Zhao, Peng Li, Jie Zhou, Xu Sun</span>
            <span class="venue">Findings of EMNLP 2022</span>
            <span class="links">
              <a class="code" href="https://aclanthology.org/2022.findings-emnlp.477">[Paper]</a>
              <a class="code" href="https://github.com/lancopku/MUKI">[Code]</a>
            </span>
          </li>
          <li>
            <a class="title-link" href="https://aclanthology.org/2021.emnlp-main.31/">Dynamic Knowledge Distillation for Pre-trained Language Models</a>
            <span class="authors"><strong>Lei Li</strong>, Yankai Lin, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun</span>
            <span class="venue">EMNLP 2021 (Oral)</span>
            <span class="links">
              <a class="code" href="https://aclanthology.org/2021.emnlp-main.31/">[Paper]</a>
              <a class="code" href="https://github.com/lancopku/DynamicKD">[Code]</a>
            </span>
          </li>
          <li>
            <a class="title-link" href="https://aclanthology.org/2021.findings-emnlp.43">CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade</a>
            <span class="authors"><strong>Lei Li</strong>, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun</span>
            <span class="venue">Findings of EMNLP 2021</span>
            <span class="links">
              <a class="code" href="https://aclanthology.org/2021.findings-emnlp.43">[Paper]</a>
              <a class="code" href="https://github.com/lancopku/cascadebert">[Code]</a>
            </span>
          </li>
        </ul>

        <h3>Knowledge-Enhanced NLP</h3>
        <ul class="pub-list">
          <li>
            <a class="title-link" href="https://arxiv.org/pdf/2105.13607.pdf">Alleviating the Knowledge-Language Inconsistency: A Study for Deep Commonsense Knowledge</a>
            <span class="authors">Yi Zhang #, <strong>Lei Li #</strong>, Yunfang Wu, Qi Su, Xu Sun</span>
            <span class="venue">IEEE TASLP</span>
            <span class="links">
              <a class="code" href="https://arxiv.org/pdf/2105.13607.pdf">[Paper]</a>
            </span>
          </li>
          <li>
            <a class="title-link" href="https://aclanthology.org/P19-1193/">Enhancing Topic-to-essay Generation with External Commonsense Knowledge</a>
            <span class="authors">Pengcheng Yang #, <strong>Lei Li #</strong>, Fuli Luo, Tianyu Liu, Xu Sun</span>
            <span class="venue">ACL 2019</span>
            <span class="links">
              <a class="code" href="https://aclanthology.org/P19-1193/">[Paper]</a>
            </span>
          </li>
        </ul>

      </div>

      <!-- Academic Service -->
      <h2>Academic Service</h2>
      <ul class="service">
        <li><strong>Area Chair / Action Editor:</strong> ACL ARR (2024 &ndash; Now)</li>
        <li><strong>Reviewer / Program Committee:</strong> IJCV, ACM CSUR, TASLP, AAAI 2025, NeurIPS (2023 &ndash; ), COLM (2024 &ndash; ), ICML (2024 &ndash; ), CVPR (2024 &ndash; ), ICLR (2024 &ndash; Now), ACL (2020 &ndash; 2023), EMNLP (2019 &ndash; 2023)</li>
        <li><strong>Teaching Assistant:</strong> Smart Phone Apps Development (2024, HKU) &middot; Machine Learning in Trading and Finance (2023 Fall, HKU) &middot; Computational Linguistics (2021 Fall, PKU)</li>
      </ul>

      <!-- Invited Talks -->
      <h2>Invited Talks</h2>
      <ul class="service">
        <li><span class="date">[2024/10]</span> <a href="https://space.bilibili.com/503316308">AI TIME</a> &mdash; VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Model Alignment <a class="code" href="https://www.bilibili.com/video/BV1FDyBYbEw1">[Video]</a></li>
        <li><span class="date">[2023/12]</span> <a href="https://nice-nlp.github.io/">NICE-NLP</a> &mdash; Can Large Language Models Understand Physical Concepts? <a class="code" href="https://www.bilibili.com/video/BV1Mi4y1z7q2">[Video]</a></li>
        <li><span class="date">[2023/03]</span> <a href="https://blcu-lcclab.cn/">LCC-Lab, BLCU</a> &mdash; A Survey on In-context Learning <a class="code" href="https://www.bilibili.com/video/BV1aT411k7TF">[Video]</a></li>
      </ul>

      <!-- Awards -->
      <h2>Awards</h2>
      <ul class="awards">
        <li><i class="fa-solid fa-medal"></i> Tencent Qingyun Scholarship (inaugural, 15 nationwide), 2026</li>
        <li><i class="fa-solid fa-award"></i> Outstanding Area Chair, EMNLP, 2025</li>
        <li><i class="fa-solid fa-trophy"></i> Best Long Paper Award, EMNLP, 2023</li>
        <li><i class="fa-solid fa-medal"></i> National Scholarship, Peking University, 2021</li>
        <li><i class="fa-solid fa-medal"></i> National Scholarship, Xidian University, 2019</li>
        <li><i class="fa-solid fa-award"></i> Best Demo Award, <a href="https://deecamp.com/">DeeCamp</a>, 2018</li>
      </ul>

    </section>

    <footer>
      <p><a href="#top">&uarr; Back to top</a></p>
      <p>&copy; Lei Li 2025</p>
    </footer>

  </div>
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      var btn = document.getElementById('older-pub-btn');
      var content = document.getElementById('older-pub');
      var open = false;
      btn.addEventListener('click', function() {
        open = !open;
        content.style.display = open ? 'block' : 'none';
        btn.innerHTML = open ? '\u25BC Older Publications' : '\u25B6 Older Publications';
      });
    });
  </script>
</body>
</html>
